# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# streamlit_app.py     â† nom reconnu par Streamlitâ€¯Cloud
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""
Appairage codesÂ M2 â‡† familles client
------------------------------------
â€¢ CSV / XLSX robustes, lecture *lazy* (dtype=str, low_memory=True).
â€¢ Cache DataFrame -> redÃ©marrage instantanÃ© aprÃ¨s un 1áµ‰Ê³ run.
â€¢ Barre dâ€™Ã©tat dÃ©taillÃ©e : tu sais exactement oÃ¹ Ã§a charge.
"""

from __future__ import annotations
import csv, io, os, psutil
from datetime import datetime
from pathlib import Path
from typing import List

import pandas as pd
import streamlit as st

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  RÃ©glages rapides  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEBUG_SAMPLE_ROWS: int | None = None   # 10_000 pour debug, None sinon

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Mise en page  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="Appairage M2", page_icon="ðŸ› ï¸", layout="wide")
st.title("ðŸ› ï¸Â Appairage codesÂ M2 / familles client")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Helpers I/O  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _read_csv(buffer: io.BytesIO) -> pd.DataFrame:
    """DÃ©tecte encodage + sÃ©parateur, lit le CSV en dtype=str."""
    for enc in ("utf-8", "latin1", "cp1252"):
        buffer.seek(0)
        try:
            sample = buffer.read(2048).decode(enc, errors="ignore")
            sep = csv.Sniffer().sniff(sample, delimiters=";,|\t").delimiter
            buffer.seek(0)
            return pd.read_csv(
                buffer, sep=sep, encoding=enc, dtype=str,
                low_memory=True, engine="python", on_bad_lines="skip"
            )
        except (csv.Error, UnicodeDecodeError, pd.errors.ParserError):
            continue
    raise ValueError("Impossible de dÃ©tecter l'encodage / sÃ©parateur.")

@st.cache_data(show_spinner=False, hash_funcs={io.BytesIO: lambda _: None})
def load_df(upload) -> pd.DataFrame:
    """Charge un UploadedFile â†’ DataFrame, rÃ©sultat mis en cache."""
    suffix = Path(upload.name.lower()).suffix
    data = upload.getvalue()          # bytes immuables (clÃ© du cache)
    buf = io.BytesIO(data)

    if suffix == ".csv":
        return _read_csv(buf)

    if suffix == ".xlsx":
        buf.seek(0)
        return pd.read_excel(buf, engine="openpyxl", dtype=str)

    raise ValueError(f"Extension non prise en chargeâ€¯: {suffix}")

def to_m2(series: pd.Series) -> pd.Series:
    """Formate la sÃ©rie en chaÃ®ne Ã  6 chiffres (zeroâ€‘padding)."""
    return series.astype(str).str.zfill(6)

def add_cols(df: pd.DataFrame, ref_idx: int, m2_idx: int,
             ref_name: str, m2_name: str) -> pd.DataFrame:
    """Ajoute/normalise colonne RÃ©f. + colonne M2 nÂ°."""
    out = df.iloc[:, [ref_idx-1, m2_idx-1]].copy()
    out.columns = [ref_name, m2_name]
    out[m2_name] = to_m2(out[m2_name])
    return out

def ram_usage() -> str:
    rss = psutil.Process(os.getpid()).memory_info().rss / 1_048_576
    return f"{rss:,.0f}â€¯Mo"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Upload interface  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LOTS = {
    "old": ("DonnÃ©es Nâ€‘1",   "RÃ©f. client",   "M2 ancien"),
    "new": ("DonnÃ©es N",     "RÃ©f. client",   "M2 nouveau"),
    "map": ("Table mapping", "M2 ancien",     "Code famille client"),
}

for key in LOTS:
    st.session_state.setdefault(f"{key}_files", [])   # type: List[st.UploadedFile]
    st.session_state.setdefault(f"{key}_names", [])

cols = st.columns(3)
for (key, (title, lab_ref, lab_val)), col in zip(LOTS.items(), cols):
    with col:
        st.subheader(title)
        uploads = st.file_uploader(
            "DÃ©pose fichier(s)â€¦", type=("csv", "xlsx"),
            accept_multiple_files=True, key=f"uploader_{key}"
        )
        if uploads:
            fresh = 0
            for up in uploads:
                if up.name not in st.session_state[f"{key}_names"]:
                    st.session_state[f"{key}_files"].append(up)
                    st.session_state[f"{key}_names"].append(up.name)
                    fresh += 1
            if fresh:
                st.success(f"{fresh} fichier(s) ajoutÃ©(s)")

        st.number_input(lab_ref, 1, 50, 1, key=f"{key}_ref")
        st.number_input(lab_val, 1, 50, 2, key=f"{key}_val")
        st.caption(f"{len(st.session_state[f'{key}_files'])} fichier(s)Â | RAMâ€¯: {ram_usage()}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Traitement  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.button("ðŸ”—  Lancer lâ€™appairage"):
    if not all(st.session_state[f"{k}_files"] for k in LOTS):
        st.warning("Merci de charger les trois lots avant de continuer.")
        st.stop()

    dfs: dict[str, pd.DataFrame] = {}
    for key in LOTS:
        with st.status(f"Lecture & concat {key.upper()}â€¦", expanded=False):
            parts = [load_df(up) for up in st.session_state[f"{key}_files"]]
            if any(df is None for df in parts):
                st.error("Lecture impossible dans un fichier.")
                st.stop()
            df_cat = pd.concat(parts, ignore_index=True).drop_duplicates()
            if DEBUG_SAMPLE_ROWS:
                df_cat = df_cat.head(DEBUG_SAMPLE_ROWS)
            dfs[key] = df_cat
            st.write(f"â†’ {len(df_cat):,} lignes (RAMâ€¯: {ram_usage()})")

    # VÃ©rification des index entrÃ©s
    for key, df in dfs.items():
        ref_i, val_i = st.session_state[f"{key}_ref"], st.session_state[f"{key}_val"]
        if not (1 <= ref_i <= df.shape[1] and 1 <= val_i <= df.shape[1]):
            st.error(f"Indice hors plage pour le lot {key.upper()}.")
            st.stop()

    # PrÃ©â€‘formatage
    old_df = add_cols(dfs["old"], st.session_state["old_ref"], st.session_state["old_val"],
                      "Reference", "M2_ancien")
    new_df = add_cols(dfs["new"], st.session_state["new_ref"], st.session_state["new_val"],
                      "Reference", "M2_nouveau")

    map_df = dfs["map"].iloc[:, [st.session_state["map_ref"]-1,
                                 st.session_state["map_val"]-1]].copy()
    map_df.columns = ["M2_ancien", "Code_famille_Client"]
    map_df["M2_ancien"] = to_m2(map_df["M2_ancien"])

    # Fusion
    with st.status("Fusionâ€¦", expanded=False):
        merged = new_df.merge(old_df[["Reference", "M2_ancien"]], on="Reference", how="outer")
        merged = merged.merge(map_df, on="M2_ancien", how="left")
        st.write(f"Fusion OKÂ : {len(merged):,} lignes (RAMâ€¯: {ram_usage()})")

    st.dataframe(merged.head())

    # Appairages par majoritÃ©
    family_map = (
        merged.groupby("M2_nouveau")["Code_famille_Client"]
        .agg(lambda s: s.value_counts().idxmax() if s.notna().any() else pd.NA)
        .reset_index()
    )
    m2_map = (
        merged.groupby("M2_nouveau")["M2_ancien"]
        .agg(lambda s: s.value_counts().idxmax() if s.notna().any() else pd.NA)
        .reset_index()
    )

    dstr = datetime.today().strftime("%y%m%d")
    st.download_button(
        "â¬‡ï¸Â Appairage M2â€¯â†’â€¯Famille",
        family_map.to_csv(index=False, sep=";"),
        file_name=f"appairage_M2_CodeFamilleClient_{dstr}.csv",
        mime="text/csv",
    )
    st.download_button(
        "â¬‡ï¸Â Mise Ã Â jour M2",
        m2_map[["M2_ancien", "M2_nouveau"]].to_csv(index=False, sep=";"),
        file_name=f"M2_MisAJour_{dstr}.csv",
        mime="text/csv",
    )

    st.success("âœ“ Fichiers prÃªts au tÃ©lÃ©chargement.")
